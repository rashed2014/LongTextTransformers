{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BigBirdForMultipleChoice, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "from torch.utils.data import random_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import random_split\n",
        "import json\n",
        "import os\n",
        "import torch_optimizer as optim\n",
        "from tqdm.auto import tqdm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/azureuser/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1699815717326
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#initialize model\n",
        "model_name = 'google/bigbird-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Initialize the model\n",
        "model = BigBirdForMultipleChoice.from_pretrained(model_name).to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BigBirdForMultipleChoice were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815719681
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "import json\n",
        "with open('data/wikihop/train.json', 'r') as file:\n",
        "    wikihop_data = json.load(file)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726044
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this method is to truncate the context size by relevance. \n",
        "import re\n",
        "\n",
        "def extract_context_windows(text, candidates, window_size=45):\n",
        "    # Pattern to match a word\n",
        "    word_pattern = r'\\b\\w+\\b'\n",
        "    # Combine all candidates into a single regex pattern\n",
        "    candidates_pattern = '|'.join(re.escape(candidate) for candidate in candidates)\n",
        "    # Compile a case-insensitive regex pattern\n",
        "    pattern = re.compile(candidates_pattern, re.IGNORECASE)\n",
        "    \n",
        "    # Initialize an empty list to hold all the windows\n",
        "    windows = []\n",
        "\n",
        "    # Find all matches of the pattern\n",
        "    for match in pattern.finditer(text):\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end()\n",
        "\n",
        "        # Find words around the candidate match\n",
        "        words_before = re.findall(word_pattern, text[:start_pos])[-window_size:]\n",
        "        words_after = re.findall(word_pattern, text[end_pos:])[:window_size]\n",
        "        \n",
        "        # Combine words before, the candidate, and words after into a window\n",
        "        window = ' '.join(words_before + [match.group()] + words_after)\n",
        "        windows.append(window)\n",
        "\n",
        "    # Combine all windows into a new context\n",
        "    new_context = ' '.join(windows)\n",
        "    return new_context"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726138
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WikiHopDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=1024):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Extract the question, candidates, and supports for the current index\n",
        "        question = item['query']\n",
        "        supports = ' '.join(item['supports'])\n",
        "        candidates = item['candidates']\n",
        "        correct_answer = item['answer']\n",
        "\n",
        "        # Limit candidates to a max of 10\n",
        "        if len(candidates) > 7:\n",
        "            candidates = candidates[:7]\n",
        "\n",
        "        # Ensure the correct answer is always included\n",
        "        if correct_answer not in candidates:\n",
        "            candidates.insert(0, correct_answer)  \n",
        "\n",
        "        # Shuffle candidates\n",
        "        random.shuffle(candidates)  \n",
        "\n",
        "        # Extract only relevant supports\n",
        "        full_context = extract_context_windows(supports, candidates)\n",
        "\n",
        "        # Combine the question with the full context\n",
        "        combined_context = question + \" \" + full_context\n",
        "\n",
        "        # Tokenize the combined context (query + supports)\n",
        "        context_max_len = self.max_length - int(self.max_length * 0.1)\n",
        "        context_encoding = self.tokenizer.encode_plus(combined_context, \n",
        "                                                    add_special_tokens=True, \n",
        "                                                    max_length=context_max_len, \n",
        "                                                    padding='max_length',\n",
        "                                                    truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Tokenize each candidate\n",
        "        candidate_max_len = int(self.max_length * 0.01)\n",
        "        candidates_encoding = [self.tokenizer.encode_plus(candidate, \n",
        "                                                        add_special_tokens=False, \n",
        "                                                        max_length=candidate_max_len, \n",
        "                                                        padding='max_length',\n",
        "                                                        truncation=True, return_tensors=\"pt\") \n",
        "                            for candidate in candidates]\n",
        "\n",
        "        # Combine context with each candidate\n",
        "        input_ids = torch.cat([context_encoding['input_ids'].repeat(len(candidates), 1), \n",
        "                            torch.stack([c['input_ids'].squeeze(0) for c in candidates_encoding])], dim=1)\n",
        "        attention_mask = torch.cat([context_encoding['attention_mask'].repeat(len(candidates), 1), \n",
        "                                    torch.stack([c['attention_mask'].squeeze(0) for c in candidates_encoding])], dim=1)\n",
        "\n",
        "        # Get the label (index of the correct answer)\n",
        "        label = torch.tensor(candidates.index(correct_answer))\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': label\n",
        "        }\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726223
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Separate input_ids, attention_masks, token_type_ids, and labels\n",
        "    all_input_ids = [item['input_ids'] for item in batch]\n",
        "    all_attention_masks = [item['attention_mask'] for item in batch]\n",
        "    all_labels = [item['labels'] for item in batch]\n",
        "\n",
        "    # Find the maximum number of choices and maximum sequence length\n",
        "    max_num_choices = max(input_ids.shape[0] for input_ids in all_input_ids)\n",
        "    max_seq_len = max(input_ids.shape[1] for input_ids in all_input_ids)\n",
        "\n",
        "    # Pad each choice in each batch item to the maximum sequence length for input_ids\n",
        "    padded_input_ids = [pad_sequence(item, batch_first=True, padding_value=tokenizer.pad_token_id).view(-1, max_seq_len)[:max_num_choices] \n",
        "                        for item in all_input_ids]\n",
        "\n",
        "    # Pad each batch item to have the same number of choices for input_ids\n",
        "    padded_input_ids = pad_sequence(padded_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    # Repeat the same padding process for attention_mask\n",
        "    padded_attention_masks = [pad_sequence(item, batch_first=True, padding_value=0).view(-1, max_seq_len)[:max_num_choices] \n",
        "                              for item in all_attention_masks]\n",
        "    padded_attention_masks = pad_sequence(padded_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "    # Pad labels to the batch size\n",
        "    labels = torch.tensor(all_labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_input_ids,\n",
        "        'attention_mask': padded_attention_masks,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726330
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare data\n",
        "wikihop_data = wikihop_data\n",
        "train_size = int(0.8 * len(wikihop_data))  # 80% of dataset for training\n",
        "val_size = len(wikihop_data) - train_size  # Remaining 20% for validation\n",
        "train_data, val_data = random_split(wikihop_data, [train_size, val_size])"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726415
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikiHop_train_dataset = WikiHopDataset(train_data, tokenizer, max_length=2500)\n",
        "wikiHop_val_dataset = WikiHopDataset(val_data, tokenizer, max_length=2500)\n",
        "\n",
        "train_loader = DataLoader(wikiHop_train_dataset, batch_size=4, collate_fn=custom_collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(wikiHop_val_dataset, batch_size=4, collate_fn=custom_collate_fn, pin_memory=True)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726507
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test batch \n",
        "\n",
        "# Fetch a batch from the DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Extract input_ids, attention_mask, and labels from the batch\n",
        "input_ids = batch['input_ids']\n",
        "attention_mask = batch['attention_mask']\n",
        "labels = batch['labels']\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of input_ids: {input_ids.shape}\")\n",
        "print(f\"Shape of attention_mask: {attention_mask.shape}\")\n",
        "print(f\"Shape of labels: {labels.shape}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape of input_ids: torch.Size([4, 8, 2275])\nShape of attention_mask: torch.Size([4, 8, 2275])\nShape of labels: torch.Size([4])\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726716
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your DataLoader is named `data_loader` and the BigBird model's max token size is `max_length`\n",
        "max_length = 1024\n",
        "# Fetch a batch from the DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Extract input_ids from the batch\n",
        "input_ids = batch['input_ids']\n",
        "batch_size, num_choices, seq_length = input_ids.shape\n",
        "\n",
        "# Iterate over each item and choice in the batch\n",
        "for i in range(batch_size):\n",
        "    for j in range(num_choices):\n",
        "        token_length = (input_ids[i, j] != tokenizer.pad_token_id).sum()\n",
        "        print(f\"Token length for item {i}, choice {j}: {token_length}\")\n",
        "\n",
        "        # Check if token length exceeds the maximum allowed length\n",
        "        if token_length > max_length:\n",
        "            print(f\"Warning: Token length for item {i}, choice {j} exceeds the maximum length of {max_length}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Token length for item 0, choice 0: 598\nToken length for item 0, choice 1: 593\nToken length for item 0, choice 2: 597\nToken length for item 0, choice 3: 593\nToken length for item 0, choice 4: 592\nToken length for item 0, choice 5: 0\nToken length for item 0, choice 6: 0\nToken length for item 0, choice 7: 0\nToken length for item 1, choice 0: 2252\nWarning: Token length for item 1, choice 0 exceeds the maximum length of 1024\nToken length for item 1, choice 1: 2251\nWarning: Token length for item 1, choice 1 exceeds the maximum length of 1024\nToken length for item 1, choice 2: 2251\nWarning: Token length for item 1, choice 2 exceeds the maximum length of 1024\nToken length for item 1, choice 3: 2252\nWarning: Token length for item 1, choice 3 exceeds the maximum length of 1024\nToken length for item 1, choice 4: 2257\nWarning: Token length for item 1, choice 4 exceeds the maximum length of 1024\nToken length for item 1, choice 5: 2251\nWarning: Token length for item 1, choice 5 exceeds the maximum length of 1024\nToken length for item 1, choice 6: 2254\nWarning: Token length for item 1, choice 6 exceeds the maximum length of 1024\nToken length for item 1, choice 7: 2251\nWarning: Token length for item 1, choice 7 exceeds the maximum length of 1024\nToken length for item 2, choice 0: 526\nToken length for item 2, choice 1: 528\nToken length for item 2, choice 2: 525\nToken length for item 2, choice 3: 526\nToken length for item 2, choice 4: 525\nToken length for item 2, choice 5: 0\nToken length for item 2, choice 6: 0\nToken length for item 2, choice 7: 0\nToken length for item 3, choice 0: 2251\nWarning: Token length for item 3, choice 0 exceeds the maximum length of 1024\nToken length for item 3, choice 1: 2257\nWarning: Token length for item 3, choice 1 exceeds the maximum length of 1024\nToken length for item 3, choice 2: 2251\nWarning: Token length for item 3, choice 2 exceeds the maximum length of 1024\nToken length for item 3, choice 3: 2252\nWarning: Token length for item 3, choice 3 exceeds the maximum length of 1024\nToken length for item 3, choice 4: 2251\nWarning: Token length for item 3, choice 4 exceeds the maximum length of 1024\nToken length for item 3, choice 5: 2251\nWarning: Token length for item 3, choice 5 exceeds the maximum length of 1024\nToken length for item 3, choice 6: 2251\nWarning: Token length for item 3, choice 6 exceeds the maximum length of 1024\nToken length for item 3, choice 7: 2251\nWarning: Token length for item 3, choice 7 exceeds the maximum length of 1024\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815726987
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checkpoint save method\n",
        "#model, optimizer, scaler, scheduler, epoch, step, checkpoint_filepath\n",
        "def save_checkpoint(model, optimizer, scaler, scheduler, epoch, step, filepath):\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815727095
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation method\n",
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # No need to use autocast in validation as it is beneficial during backward pass which doesn't occur in validation\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, \n",
        "                            labels=labels)\n",
        "            loss = outputs.loss  \n",
        "            if loss.dim() > 0:\n",
        "                loss = loss.mean()          \n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_loader)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815727190
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"{torch.cuda.device_count()} GPUs available. Using Data Parallel.\")\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "#training and validate\n",
        "checkpoint_interval = 100 \n",
        "\n",
        "#save_path\n",
        "save_path = \"models/BigBird\"\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-4  # You can experiment with this value\n",
        "adam_epsilon = 1e-8\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = num_training_steps // 10  # 10% of training steps as warm-up\n",
        "\n",
        "# Prepare the model and optimizers\n",
        "model.to(device)\n",
        "base_optimizer = optim.RAdam(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "optimizer = optim.Lookahead(base_optimizer, k=5, alpha=0.5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training loop\n",
        "accumulation_steps = 4  # for example, accumulate gradients over 4 forward passes\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    optimizer.zero_grad()  # Move zero_grad() outside the inner loop\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            \n",
        "            loss = outputs.loss / accumulation_steps  # Scale loss\n",
        "            if loss.dim() > 0:\n",
        "                loss = loss.mean()   \n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0 or step + 1 == len(train_loader):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()  # Zero the gradients after optimizer step\n",
        "\n",
        "            scheduler.step()  # Update the learning rate\n",
        "\n",
        "        train_loss += loss.item() * accumulation_steps  # Unscaled loss for logging\n",
        "\n",
        "        if (step + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_filename = f\"checkpoint_epoch_{epoch+1}_step_{step+1}.pt\"\n",
        "            checkpoint_filepath = os.path.join(save_path, checkpoint_filename)\n",
        "            # Save checkpoint function needs to be updated to handle the new optimizer\n",
        "            save_checkpoint(model, optimizer, scaler, scheduler, epoch, step, checkpoint_filepath)\n",
        "\n",
        "    average_train_loss = train_loss / len(train_loader)\n",
        "    val_loss = validate(model, val_loader, device)  \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Train Loss: {average_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "final_model_path = os.path.join(save_path, \"BigBird_10000_rows_model.pt\")\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Training complete. Final model saved to {final_model_path}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "8 GPUs available. Using Data Parallel.\nEpoch 1/5 completed. Train Loss: 1.3592, Val Loss: 0.6534\nEpoch 2/5 completed. Train Loss: 0.5682, Val Loss: 0.5422\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Training Epoch 1/5:   0%|          | 0/8748 [00:00<?, ?it/s]2023-11-12 19:02:13.060244: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-11-12 19:02:13.895651: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-11-12 19:02:13.895725: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-11-12 19:02:13.895731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraining Epoch 1/5:   0%|          | 3/8748 [00:17<11:51:31,  4.88s/it]/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\nTraining Epoch 1/5: 100%|██████████| 8748/8748 [7:12:10<00:00,  2.96s/it]   \nTraining Epoch 2/5: 100%|██████████| 8748/8748 [7:13:26<00:00,  2.97s/it]   \nTraining Epoch 3/5:  61%|██████    | 5302/8748 [4:25:50<9:05:52,  9.50s/it] \rTraining Epoch 3/5:  65%|██████▍   | 5677/8748 [4:44:03<2:14:41,  2.63s/it]"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699815673746
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}