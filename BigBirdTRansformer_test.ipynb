{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import BigBirdForMultipleChoice, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from tqdm.auto import tqdm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890915352
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#initialize model\n",
        "model_name = 'google/bigbird-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Initialize the model\n",
        "model = BigBirdForMultipleChoice.from_pretrained(model_name).to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BigBirdForMultipleChoice were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890918629
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "import json\n",
        "with open('data/wikihop/dev.json', 'r') as file:\n",
        "    wikihop_dev = json.load(file)\n",
        "with open('data/wikihop/train.json', 'r') as file:\n",
        "    wikihop_train = json.load(file)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890925587
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Processing\n",
        "#this method is to truncate the context size by relevance. \n",
        "import re\n",
        "\n",
        "def extract_context_windows(text, candidates, window_size=45):\n",
        "    # Pattern to match a word\n",
        "    word_pattern = r'\\b\\w+\\b'\n",
        "    # Combine all candidates into a single regex pattern\n",
        "    candidates_pattern = '|'.join(re.escape(candidate) for candidate in candidates)\n",
        "    # Compile a case-insensitive regex pattern\n",
        "    pattern = re.compile(candidates_pattern, re.IGNORECASE)\n",
        "    \n",
        "    # Initialize an empty list to hold all the windows\n",
        "    windows = []\n",
        "\n",
        "    # Find all matches of the pattern\n",
        "    for match in pattern.finditer(text):\n",
        "        start_pos = match.start()\n",
        "        end_pos = match.end()\n",
        "\n",
        "        # Find words around the candidate match\n",
        "        words_before = re.findall(word_pattern, text[:start_pos])[-window_size:]\n",
        "        words_after = re.findall(word_pattern, text[end_pos:])[:window_size]\n",
        "        \n",
        "        # Combine words before, the candidate, and words after into a window\n",
        "        window = ' '.join(words_before + [match.group()] + words_after)\n",
        "        windows.append(window)\n",
        "\n",
        "    # Combine all windows into a new context\n",
        "    new_context = ' '.join(windows)\n",
        "    return new_context\n",
        "\n",
        "\n",
        "class WikiHopDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=1024):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # Extract the question, candidates, and supports for the current index\n",
        "        question = item['query']\n",
        "        supports = ' '.join(item['supports'])\n",
        "        candidates = item['candidates']\n",
        "        correct_answer = item['answer']\n",
        "\n",
        "        # Limit candidates to a max of 10\n",
        "        if len(candidates) > 7:\n",
        "            candidates = candidates[:7]\n",
        "\n",
        "        # Ensure the correct answer is always included\n",
        "        if correct_answer not in candidates:\n",
        "            candidates.insert(0, correct_answer)  \n",
        "\n",
        "        # Shuffle candidates\n",
        "        random.shuffle(candidates)  \n",
        "\n",
        "        # Extract only relevant supports\n",
        "        full_context = extract_context_windows(supports, candidates)\n",
        "\n",
        "        # Combine the question with the full context\n",
        "        combined_context = question + \" \" + full_context\n",
        "\n",
        "        # Tokenize the combined context (query + supports)\n",
        "        context_max_len = int(self.max_length * 0.98)\n",
        "        context_encoding = self.tokenizer.encode_plus(combined_context, \n",
        "                                                    add_special_tokens=True, \n",
        "                                                    max_length=context_max_len, \n",
        "                                                    padding='max_length',\n",
        "                                                    truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Tokenize each candidate\n",
        "        candidate_max_len = int(self.max_length * 0.02)\n",
        "        candidates_encoding = [self.tokenizer.encode_plus(candidate, \n",
        "                                                        add_special_tokens=False, \n",
        "                                                        max_length=candidate_max_len, \n",
        "                                                        padding='max_length',\n",
        "                                                        truncation=True, return_tensors=\"pt\") \n",
        "                            for candidate in candidates]\n",
        "\n",
        "        # Combine context with each candidate\n",
        "        input_ids = torch.cat([context_encoding['input_ids'].repeat(len(candidates), 1), \n",
        "                            torch.stack([c['input_ids'].squeeze(0) for c in candidates_encoding])], dim=1)\n",
        "        attention_mask = torch.cat([context_encoding['attention_mask'].repeat(len(candidates), 1), \n",
        "                                    torch.stack([c['attention_mask'].squeeze(0) for c in candidates_encoding])], dim=1)\n",
        "\n",
        "        # Get the label (index of the correct answer)\n",
        "        label = torch.tensor(candidates.index(correct_answer))\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': label\n",
        "        }\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    # Separate input_ids, attention_masks, token_type_ids, and labels\n",
        "    all_input_ids = [item['input_ids'] for item in batch]\n",
        "    all_attention_masks = [item['attention_mask'] for item in batch]\n",
        "    all_labels = [item['labels'] for item in batch]\n",
        "\n",
        "    # Find the maximum number of choices and maximum sequence length\n",
        "    max_num_choices = max(input_ids.shape[0] for input_ids in all_input_ids)\n",
        "    max_seq_len = max(input_ids.shape[1] for input_ids in all_input_ids)\n",
        "\n",
        "    # Pad each choice in each batch item to the maximum sequence length for input_ids\n",
        "    padded_input_ids = [pad_sequence(item, batch_first=True, padding_value=tokenizer.pad_token_id).view(-1, max_seq_len)[:max_num_choices] \n",
        "                        for item in all_input_ids]\n",
        "\n",
        "    # Pad each batch item to have the same number of choices for input_ids\n",
        "    padded_input_ids = pad_sequence(padded_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    # Repeat the same padding process for attention_mask\n",
        "    padded_attention_masks = [pad_sequence(item, batch_first=True, padding_value=0).view(-1, max_seq_len)[:max_num_choices] \n",
        "                              for item in all_attention_masks]\n",
        "    padded_attention_masks = pad_sequence(padded_attention_masks, batch_first=True, padding_value=0)\n",
        "\n",
        "\n",
        "    # Pad labels to the batch size\n",
        "    labels = torch.tensor(all_labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_input_ids,\n",
        "        'attention_mask': padded_attention_masks,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890925668
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikiHop_test_dataset = WikiHopDataset(wikihop_dev[:100], tokenizer, max_length=2750)\n",
        "\n",
        "test_loader = DataLoader(wikiHop_test_dataset, batch_size=4, collate_fn=custom_collate_fn, pin_memory=True)\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890925740
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            if loss.dim() > 0:\n",
        "                loss = loss.mean()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            # print(f\"predictions: {predictions}\")\n",
        "            # print(f\"labels: {labels}\")\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    return avg_test_loss, accuracy\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1699890925795
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import LongformerForMultipleChoice\n",
        "\n",
        "#load model\n",
        "checkpoint = torch.load('models/BigBird/checkpoint_epoch_3_step_6300.pt', map_location='cpu')\n",
        "\n",
        "model = BigBirdForMultipleChoice.from_pretrained('google/bigbird-roberta-base', state_dict=checkpoint)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BigBirdForMultipleChoice were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'pooler.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.3.output.LayerNorm.bias', 'pooler.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699890958212
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Call the test method\n",
        "avg_test_loss, test_accuracy = test(model, test_loader, device)\n",
        "print(f'Average test loss: {avg_test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 25/25 [01:06<00:00,  2.65s/it]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Average test loss: 2.120200538635254\nTest accuracy: 0.08\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699891024475
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1, :500 rows, checkpoint_epoch_1_step_400.pt, training: 1000 rows, candidates: 7\n",
        "Average test loss: 2.129120014190674\n",
        "Test accuracy: 0.11\n",
        "\n",
        "Test 2, :500 rows, checkpoint_epoch_1_step_400.pt, training: 1000 rows, candidates: 4\n",
        "Average test loss: \n",
        "Test accuracy: 0.18\n",
        "\n",
        "Test 3, :1000 rows, checkpoint_epoch_1_step_5400.pt, training: all rows, candidates: 4\n",
        "Average test loss: 1.612755440711975\n",
        "Test accuracy: 0.132\n",
        "\n",
        "Test 4, :1000 rows, checkpoint_epoch_1_step_6000.pt, training: all rows, candidates: 7\n",
        "Average test loss: 2.0350936398506163\n",
        "Test accuracy: 0.262\n",
        "Test 4, :1000 rows, checkpoint_epoch_1_step_6000.pt, training: all rows, candidates: 4\n",
        "Average test loss: 1.616220552444458\n",
        "Test accuracy: 0.174\n",
        "\n",
        "Test 5, :1000 rows, checkpoint_epoch_1_step_7200.pt, training: all rows, candidates: 7\n",
        "Average test loss: 2.0654408631324768\n",
        "Test accuracy: 0.291\n",
        "\n",
        "Test 5, :1000 rows, checkpoint_epoch_1_step_7200.pt, training: all rows, candidates: 4\n",
        "Average test loss: 1.609327109336853\n",
        "Test accuracy: 0.174\n",
        "\n",
        "Test 6, :1000 rows, checkpoint_epoch_1_step_8300.pt, training: all rows, candidates: 7\n",
        "Average test loss: 2.0652692041397094\n",
        "Test accuracy: 0.21"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"combined_context: country sms braunschweig The North German Confederation was a confederation of 22 previously independent states of northern Germany with nearly 30 million inhabitants It was the first modern German nation state and previously independent states of northern Germany with nearly 30 million inhabitants It was the first modern German nation state and the basis for the later German Empire 18711918 when several south German states such as Bavaria joined Weimar Republic is an unofficial historical designation for the German state between 1919 and 1933 She was laid down in 1901 and commissioned in October 1904 at a cost of 23 983 000 marks She was named after the then Duchy of Brunswick German Braunschweig Her sister ships were Elsass Hessen Preussen and Lothringen The ship served in the II Squadron of the German fleet after commissioning though State of Brunswick was a state of the German Reich in the time of the Weimar Republic It was formed after the abolition of the Duchy of Brunswick in the course of the German Revolution of 191819 Its capital was Braunschweig Brunswick The Congress of Vienna German Wiener Kongress was a conference of Revolution of 191819 Its capital was Braunschweig Brunswick The Congress of Vienna German Wiener Kongress was a conference of ambassadors of European states chaired by Austria n statesman Klemens von Metternich and held in Vienna from November 1814 to June 1815 though the delegates had arrived and were already negotiating by remain at peace The leaders were conservatives with little use for republicanism or revolution both of which threatened to upset the status quo in Europe France lost all its recent conquests while Prussia Austria and Russia made major territorial gains Prussia added smaller German states in the west Swedish Pomerania and little use for republicanism or revolution both of which threatened to upset the status quo in Europe France lost all its recent conquests while Prussia Austria and Russia made major territorial gains Prussia added smaller German states in the west Swedish Pomerania and 60 of the Kingdom of Saxony Austria gained Prussia Austria and Russia made major territorial gains Prussia added smaller German states in the west Swedish Pomerania and 60 of the Kingdom of Saxony Austria gained Venice and much of northern Italy Russia gained parts of Poland The new Kingdom of the Netherlands had been created just months before and and much of northern Italy Russia gained parts of Poland The new Kingdom of the Netherlands had been created just months before and included formerly Austria n territory that in 1830 became Belgium The immediate background was Napoleonic France s defeat and surrender in May 1814 which brought an end to Kingdom of the Netherlands had been created just months before and included formerly Austrian territory that in 1830 became Belgium The immediate background was Napoleonic France s defeat and surrender in May 1814 which brought an end to twenty five years of nearly continuous war Negotiations continued despite the outbreak of years of nearly continuous war Negotiations continued despite the outbreak of fighting triggered by Napoleon s dramatic return from exile and resumption of power in France during the Hundred Days of MarchJuly 1815 The Congress s Final Act was signed nine days before his final defeat at Waterloo on 18 June Hundred Days of MarchJuly 1815 The Congress s Final Act was signed nine days before his final defeat at Waterloo on 18 June 1815 The German Confederation was an association of 39 German states in Central Europe created by the Congress of Vienna in 1815 to coordinate the economies of separate German weak and ineffective as well as an obstacle to the creation of a German nation state It collapsed due to the rivalry between Prussia and Austria warfare the 1848 revolution and the inability of the multiple members to compromise The Royal Navy RN is the United Kingdom s naval warfare force by the English kings from the early medieval period the first major maritime engagements were fought in the Hundred Years War against the kingdom of France The modern Royal Navy traces its origins to the early 16th century the oldest of the UK s armed services it is known as the Evangelical Lutheran Church in Brunswick It is also home to the Jägermeister distillery and houses a campus of the Ostfalia University of Applied Sciences The German Empire officially was the historical German nation state that existed from the unification of Germany in 1871 to the abdication of Kaiser Wilhelm II in November in Washington D C from November 1921 to February 1922 and it was signed by the governments of the United Kingdom the United States Japan France and Italy It limited the construction of battleships battlecruisers and aircraft carriers by the signatories The numbers of other categories of warships including cruisers destroyers categories of warships including cruisers destroyers and submarines were not limited by the treaty but those ships were limited to 10 000 tons displacement The Duchy of Brunswick was a historical German state Its capital was the city of Brunswick Braunschweig It was established as the successor state of the Principality of Brunswick Brunswick Wolfenbüttel by the Congress of Vienna in 1815 In the course of the 19th century history of Germany the duchy was part of the German Confederation the North German Confederation and from 1871 the German Empire It was disestablished after the end of World War I its territory incorporated into the Congress of Vienna in 1815 In the course of the 19th century history of Germany the duchy was part of the German Confederation the North German Confederation and from 1871 the German Empire It was disestablished after the end of World War I its territory incorporated into the Weimar Republic as the the course of the 19th century history of Germany the duchy was part of the German Confederation the North German Confederation and from 1871 the German Empire It was disestablished after the end of World War I its territory incorporated into the Weimar Republic as the Free State of Brunswick World War\""
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699891024553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \". join(wikihop_dev[1000]['candidates'][1])\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Count the number of tokens\n",
        "num_tokens = len(tokens)\n",
        "\n",
        "print(\"Number of tokens:\", num_tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Number of tokens: 16\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1699891024623
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}